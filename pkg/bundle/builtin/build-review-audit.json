{
  "name": "build-review-audit",
  "description": "Multi-model workflow: build, review, improve, test, and audit any software project",
  "inputs": [
    {"name": "output_dir", "required": false, "description": "Base directory for generated files (default from settings)"},
    {"name": "project_name", "required": true, "description": "Project subdirectory name"},
    {"name": "task", "required": true, "description": "What to build"}
  ],
  "steps": [
    {
      "name": "build",
      "tool": "claude",
      "task": "${inputs.task}\n\nRequirements:\n- Create well-structured, production-ready code\n- Include proper error handling and input validation\n- Add a README with usage instructions\n- Create sample files or test data as appropriate\n\nSave code to: ${inputs.output_dir}/${inputs.project_name}/src/\nSave samples/test data to: ${inputs.output_dir}/${inputs.project_name}/samples/\nSave README to: ${inputs.output_dir}/${inputs.project_name}/README.md"
    },
    {
      "name": "review",
      "tool": "gemini",
      "task": "Review the code that was just created.\n\nContext:\n${steps.build.stdout}\n\nYour task:\n1. Read all code in ${inputs.output_dir}/${inputs.project_name}/src/\n2. Identify and FIX any obvious bugs directly\n3. Document larger suggestions for the next reviewer\n\nWrite a review covering:\n- Bugs/issues you fixed (before/after)\n- Suggestions for improvement\n- Security concerns\n- Code quality observations\n\nSave to: ${inputs.output_dir}/${inputs.project_name}/review.md"
    },
    {
      "name": "improve",
      "tool": "claude",
      "task": "Review the code review and implement improvements.\n\nReview:\n${steps.review.stdout}\n\n1. Read code in ${inputs.output_dir}/${inputs.project_name}/src/\n2. Read review at ${inputs.output_dir}/${inputs.project_name}/review.md\n3. For each suggestion: evaluate, accept/reject with reasoning, implement if accepted\n\nSave decision log to: ${inputs.output_dir}/${inputs.project_name}/decisions.md"
    },
    {
      "name": "test",
      "tool": "gemini",
      "task": "Test the implementation.\n\nContext:\n${steps.improve.stdout}\n\n1. Navigate to ${inputs.output_dir}/${inputs.project_name}/\n2. Run the code against sample inputs\n3. Verify it works correctly\n4. Note any issues or UX improvements\n\nSave test results to: ${inputs.output_dir}/${inputs.project_name}/test-results.md"
    },
    {
      "name": "audit",
      "tool": "claude",
      "model": "opus",
      "task": "Final audit and grading.\n\nContext:\n${steps.test.stdout}\n\n1. Review test results at ${inputs.output_dir}/${inputs.project_name}/test-results.md\n2. Read all code in ${inputs.output_dir}/${inputs.project_name}/src/\n3. Run your own verification\n4. Audit for: bugs, security, code quality, architecture, documentation\n\nGrade using this rubric (bonus points allowed for exceptional work):\n- functionality (20 max): Does it work as specified?\n- code_quality (20 max): Clean, readable, documented\n- security (10 max): No vulnerabilities\n- user_experience (20 max): Intuitive, helpful errors\n- architecture (10 max): Clean separation, maintainable\n- testing (10 max): Verification coverage\n- innovation (5 max): Creative solutions\n- documentation (5 max): README, comments, guides\n\nWrite a comprehensive final-report.md including:\n- Executive summary\n- What was built\n- Strengths of the implementation\n- Weaknesses and areas for improvement\n- Multi-model collaboration analysis\n- Final grade with detailed justification\n- Recommendations for future development\n\nIMPORTANT: At the END of your report, output this exact JSON block with your scores:\n```json\n{\"grade\":{\"score\":N,\"letter\":\"X\",\"functionality\":N,\"code_quality\":N,\"security\":N,\"user_experience\":N,\"architecture\":N,\"testing\":N,\"innovation\":N,\"documentation\":N}}\n```\n\nSave to: ${inputs.output_dir}/${inputs.project_name}/final-report.md"
    }
  ]
}
